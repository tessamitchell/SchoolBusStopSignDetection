{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMTbnUMgTBmyDqMyPBNGGMP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tessamitchell/SchoolBusStopSignDetection/blob/main/StopSignDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Data Setup"
      ],
      "metadata": {
        "id": "HfWljAZ8Rf13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All imports used in the code are here"
      ],
      "metadata": {
        "id": "9-bCNw7wmAgt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqVyM2CgRYkK"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "from google.colab import userdata\n",
        "\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage.feature import haar_like_feature\n",
        "from skimage.feature import hog\n",
        "\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "import joblib\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow # cv.imshow doesn't work in colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download/Import Dataset\n",
        "Code copied from Roboflow's dataset download instructions\n",
        "\n",
        "[Link to Dataset](https://app.roboflow.com/myworkspace-hr4qa/stop-signs-custom-umjoj/8)"
      ],
      "metadata": {
        "id": "Ri8Zaue-l_Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = Roboflow(api_key=userdata.get('roboflow'))\n",
        "project = rf.workspace(\"myworkspace-hr4qa\").project(\"stop-signs-custom-umjoj\")\n",
        "version = project.version(8)\n",
        "dataset = version.download(\"voc\")"
      ],
      "metadata": {
        "id": "Y7SGXjtlRueJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to load positive and negative windows.\n",
        "The base code for the XML parsing came from [ChatGPT](https://chatgpt.com/share/692615ef-8958-8010-adfd-24ddd028c3e9)"
      ],
      "metadata": {
        "id": "L_Of9ZD3eGf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_positive_windows(xml_path):\n",
        "    pos_windows=[]\n",
        "    neg_windows=[]\n",
        "\n",
        "    for xml in os.listdir(xml_path):\n",
        "      # get xml annotation file\n",
        "      if xml.endswith(\".xml\"):\n",
        "        # get image file by replacing .xml with .jpg (should have same name)\n",
        "        img_name=xml.replace('.xml','.jpg')\n",
        "        # get full file path\n",
        "        img_path = os.path.join(xml_path, img_name)\n",
        "        if not os.path.exists(img_path):\n",
        "            continue  # img file for xml file doesn't exist\n",
        "        # read image\n",
        "        img=cv.imread(img_path)\n",
        "        # parse xml for image\n",
        "        tree = ET.parse(os.path.join(xml_path, xml))\n",
        "        root = tree.getroot()\n",
        "\n",
        "        boxes = []\n",
        "        for obj in root.findall(\"object\"):\n",
        "            bbox = obj.find(\"bndbox\")\n",
        "            # get corners\n",
        "            xmin = int(bbox.find(\"xmin\").text) -1 # windows were cutting off left and top part of img\n",
        "            ymin = int(bbox.find(\"ymin\").text) -1\n",
        "            xmax = int(bbox.find(\"xmax\").text)\n",
        "            ymax = int(bbox.find(\"ymax\").text)\n",
        "            # slice image to get window\n",
        "            pw=img[ymin:ymax,xmin:xmax]\n",
        "            pw=cv.resize(pw,(64,64))\n",
        "            if obj.find(\"name\").text == \"stop sign\":\n",
        "              pos_windows.append(pw)\n",
        "            else: # text should be none so negative window\n",
        "              neg_windows.append(pw)\n",
        "\n",
        "    return pos_windows,neg_windows"
      ],
      "metadata": {
        "id": "cWi46RAIR1Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call function to load the windows"
      ],
      "metadata": {
        "id": "gKihRlkSeCTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xml_path = \"/content/Stop-Signs-Custom-8/train/\"\n",
        "pos_images,neg_images=load_positive_windows(xml_path)"
      ],
      "metadata": {
        "id": "imgIndhRR3f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature extraction"
      ],
      "metadata": {
        "id": "FfMwPPdDR7WF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for extracting features\n",
        "\n",
        "Recommended values taken from examples in documentation\n",
        "\n",
        "[local_binary_pattern() documentation](https://scikit-image.org/docs/0.25.x/api/skimage.feature.html#skimage.feature.local_binary_pattern)\n",
        "\n",
        "[local_binary_pattern() example](https://scikit-image.org/docs/0.25.x/auto_examples/features_detection/plot_local_binary_pattern.html)\n",
        "\n",
        "[hog() documentation](https://scikit-image.org/docs/0.25.x/api/skimage.feature.html#skimage.feature.hog)\n"
      ],
      "metadata": {
        "id": "fINn0fUbmmAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract lbp features with set values\n",
        "radius=3\n",
        "n_points=8*radius\n",
        "METHOD = 'ror'\n",
        "def extract_lbp(img):\n",
        "  res=local_binary_pattern(img,n_points,radius,METHOD)\n",
        "  return res.flatten()\n",
        "\n",
        "# extract hog features with set values\n",
        "def extract_hog(img):\n",
        "  feature=hog(img,orientations=9, pixels_per_cell=(16, 16),cells_per_block=(1, 1),visualize=False)\n",
        "  return feature\n",
        "\n",
        "# extract both\n",
        "def extract_features(img):\n",
        "  return np.concatenate([extract_lbp(img),extract_hog(img)])"
      ],
      "metadata": {
        "id": "EEwEGW6mR-6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actual extraction step (and some preprocessing)"
      ],
      "metadata": {
        "id": "xzVg0-GPxSkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replace images with their red color channel\n",
        "pos_images = [img[:,:,2] for img in pos_images]\n",
        "neg_images = [img[:,:,2] for img in neg_images]\n",
        "\n",
        "# get features from all windows and put in array\n",
        "X_features = [extract_features(w) for w in pos_images] + \\\n",
        "             [extract_features(w) for w in neg_images]\n",
        "\n",
        "# store labels for each extracted feature set\n",
        "y_labels   = [1] * len(pos_images) + [0] * len(neg_images)\n"
      ],
      "metadata": {
        "id": "yN7snECmSF1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to save extracted features and labels (to train model)\n",
        "joblib.dump(X_features, \"adaboost_features5.pkl\")\n",
        "joblib.dump(y_labels,\"adaboost_labels5.pkl\")"
      ],
      "metadata": {
        "id": "XBhncD2BSVNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training (Adaboost Cascade)\n"
      ],
      "metadata": {
        "id": "wMiL0ck-SXI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to load previously extracted features (if not in same runtime)\n",
        "X_features=joblib.load(\"adaboost_features.pkl\")\n",
        "y_labels=joblib.load(\"adaboost_labels.pkl\")"
      ],
      "metadata": {
        "id": "5J9Es2MWScIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual training of the model off of 70% of the data, with the other 30% used for testing\n",
        "\n",
        "[AdaBoost Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
        "\n",
        "[AdaBoost Example/Tutorial](https://www.datacamp.com/tutorial/adaboost-classifier-python) -- format of code copied from here\n"
      ],
      "metadata": {
        "id": "cxL083zppaV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.3,random_state=42) # 70% training and 30% test\n",
        "\n",
        "# Create adaboost classifer object\n",
        "abc = AdaBoostClassifier(n_estimators=50,\n",
        "                         learning_rate=1)\n",
        "# Train Adaboost Classifer\n",
        "model = abc.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
        "# Accuracy for Model 5: 0.9655172413793104"
      ],
      "metadata": {
        "id": "nc1pvxYuSgm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to store the trained model\n",
        "joblib.dump(model, \"adaboost_model5.pkl\")"
      ],
      "metadata": {
        "id": "qmJRU3EhSn-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run this cell to load a previously trained model\n",
        "model=joblib.load(\"adaboost_model5.pkl\")"
      ],
      "metadata": {
        "id": "PhswoOlWSjtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Light Detection"
      ],
      "metadata": {
        "id": "jlDBCRNpSrlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Methods used for detecting the lights in the image.  First the preprocessing and then the actual circle detection and pruning of the results\n",
        "\n",
        "[medianBlur documentation](https://docs.opencv.org/4.x/dc/dd3/tutorial_gausian_median_blur_bilateral_filter.html)\n",
        "\n",
        "[normalize documentation](https://docs.opencv.org/4.x/d2/de8/group__core__array.html#ga87eef7ee3970f86906d69a92cbf064bd)\n",
        "\n",
        "[HoughCircles documentation](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html\\#ga47849c3be0d0406ad3ca45db65a25d2d)\n"
      ],
      "metadata": {
        "id": "QFDIHZuZLglS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(img):\n",
        "  # hough works better on smoothed image\n",
        "  blurred=cv.medianBlur(img,7)\n",
        "  # print(f\"min:{blurred.min()} max:{blurred.max()}\")\n",
        "  # cv2_imshow(blurred)\n",
        "  # subtract mean from image\n",
        "  avg=int(blurred.mean())\n",
        "  med = (blurred.astype(np.float32)- avg)\n",
        "  med[med < 0] = 0\n",
        "  med=np.clip(med,0,255).astype(np.uint8)\n",
        "  # normalize new img\n",
        "  med=cv.normalize(med,None,0,255,cv.NORM_MINMAX)\n",
        "  # cv2_imshow(med)\n",
        "\n",
        "  return med\n",
        "\n",
        "# documentation https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga47849c3be0d0406ad3ca45db65a25d2d\n",
        "def houghAndValidation(img,orig):\n",
        "  # make copies to process and draw found circles on\n",
        "  img=img.copy()\n",
        "  orig=orig.copy()\n",
        "  img=preprocess(img)\n",
        "  h,w=img.shape\n",
        "  # circle radius needs to be at least 1/4 the size of the stop sign and at most 1/10 the stop sign\n",
        "  minR=int(img.shape[1]/10)\n",
        "  maxR=int(img.shape[1]/4)\n",
        "  # print(maxR)\n",
        "  # get circles\n",
        "  circles = cv.HoughCircles(img,cv.HOUGH_GRADIENT,1.5,30,\n",
        "                              param1=50,param2=20,minRadius=minR,maxRadius=maxR)\n",
        "  if circles is None:\n",
        "    return [],0\n",
        "  # print(circles)\n",
        "  circles = np.uint16(np.around(circles))\n",
        "  # print(len(circles))\n",
        "  # validation\n",
        "  count=0\n",
        "  # prune results\n",
        "  for c in circles[0]:\n",
        "    # print(f\"{type(c[0])} {type(c[1])} {type(c[2])}\")\n",
        "    # get edge pixels\n",
        "    right_edge=c[0]+c[2]\n",
        "    left_edge=c[0]-c[2]\n",
        "    top_edge=c[1]-c[2]\n",
        "    bottom_edge=c[1]+c[2]\n",
        "    # if in center half of image and in either top or bottom half of image\n",
        "    if((right_edge<(w * (3/4)) and (left_edge>(w//4)))  and ((bottom_edge<(h/2)) or (top_edge>(h/2)))):\n",
        "      # draw circle and increase count\n",
        "      cv.circle(orig, (c[0], c[1]), c[2], (0, 255, 0), 1)\n",
        "      count+=1\n",
        "    # use these lines to draw circles found that were pruned\n",
        "    # else:\n",
        "    #     cv.circle(orig, (c[0], c[1]), c[2], (255, 0, 0), 1)\n",
        "  # cv2_imshow(orig)\n",
        "  return orig,count"
      ],
      "metadata": {
        "id": "oF0B3-QgUIXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fa_rqZ6eVg5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "3Ycev1NYVO1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calls the methods above to test array of images for first stop sign and then lights if positive for stop sign"
      ],
      "metadata": {
        "id": "MS5gd6XRV8F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# takes array of images and sorts them into images with stop signs and without stop signs, and also returns an array with all the images with lights in them with the lights circled\n",
        "def test(imgs):\n",
        "  pos_images=[]\n",
        "  lights=[]\n",
        "  neg_images=[]\n",
        "  for img in imgs:\n",
        "    # cv2_imshow(img)\n",
        "    # process image\n",
        "    img=cv.resize(img,(64,64))\n",
        "    # cv2_imshow(img)\n",
        "    red=img[:,:,2]\n",
        "    # cv2_imshow(red)\n",
        "    #extract features\n",
        "    feat=extract_features(red).reshape(1,-1)\n",
        "    # if contains stop sign\n",
        "    if model.predict(feat)==1:\n",
        "        print(\"found\")\n",
        "        # get lights\n",
        "        hw,count=houghAndValidation(red,img)\n",
        "        if count>0: # lights were found\n",
        "          print(\"STOP\")\n",
        "          lights.append(hw)\n",
        "          pos_images.append(hw)\n",
        "        else: # append img either way\n",
        "          pos_images.append(img)\n",
        "    else: # add img to negatives\n",
        "        neg_images.append(img)\n",
        "  return pos_images,lights,neg_images"
      ],
      "metadata": {
        "id": "INaFMyrgViK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling testing method on array of all windows extracted from dataset."
      ],
      "metadata": {
        "id": "a8JtMV70WC_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test overall extracted windows\n",
        "test_images=np.concatenate([pos_images,neg_images])\n",
        "# test_images=pos_images[:20]\n",
        "\n",
        "pos,lights,neg=test(test_images)\n",
        "print(len(pos))\n",
        "print(len(neg))\n",
        "print(len(lights))"
      ],
      "metadata": {
        "id": "qU497V5rUUSJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}